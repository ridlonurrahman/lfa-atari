{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = np.array([1,2,3,4,5])\n",
    "phi = np.array([2,2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.dot(theta.T, phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.43941165,  0.48912863])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(low=0.1, high=0.5, size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from enduro.agent import Agent\n",
    "from enduro.action import Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, -3]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [phi_position(x), phi_action('BRAKE')]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6, 5])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(grid_opponents==1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2  2]\n"
     ]
    }
   ],
   "source": [
    "_ = np.where(grid_opponents==1)[1]-np.where(x==2)[0][0]\n",
    "if any(x<=0 and x>-3 for x in _):\n",
    "    print _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def phi_position(grid, act):\n",
    "    grid = grid.tolist()\n",
    "    if grid.index(2)>5:\n",
    "        if act == 0:\n",
    "            return 1\n",
    "        elif act == 2:\n",
    "            return 5\n",
    "        else:\n",
    "            return 1\n",
    "    elif grid.index(2)<4:\n",
    "        if act == 0:\n",
    "            return 1\n",
    "        elif act == 1:\n",
    "            return 5\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        if act == 0:\n",
    "            return 10\n",
    "        elif act == 3:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "\n",
    "def phi_action(act):\n",
    "    if act == 1 or act == 2:\n",
    "        return 2\n",
    "    elif act == 0:\n",
    "        return 5\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def phi_opponent(grid_agent, grid_opponent, act):\n",
    "    opponents = np.where(grid_opponent==1)[1]\n",
    "    agent = np.where(grid_agent==2)[0][0]\n",
    "    \n",
    "    if any(x>=0 and x<3 for x in opponents-agent):\n",
    "        #print \"WARNING\"\n",
    "        #print opponents\n",
    "        #print agent\n",
    "        if act == 2:\n",
    "            return 20\n",
    "        else:\n",
    "            return 0\n",
    "    elif any(x<=0 and x>-3 for x in opponents-agent):\n",
    "        #print \"WARNING 2\"\n",
    "        #print opponents\n",
    "        #print agent\n",
    "        if act == 1:\n",
    "            return 20\n",
    "        else:\n",
    "            return 0      \n",
    "    else:\n",
    "        if act == 3:\n",
    "            return 0\n",
    "        else:\n",
    "            return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1000: 4\n",
      "1/2000: 12\n",
      "1/3000: 25\n",
      "1/4000: 44\n",
      "1/5000: 75\n",
      "1/6000: 91\n",
      "2/1000: 4\n",
      "2/2000: 14\n",
      "2/3000: 18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-286-f6dcbe834bd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionApproximationAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'Total reward: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ridlo/rl/rl-cw2/enduro/agent.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, learn, episodes, draw)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0;31m# Update the environment grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mroad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_image\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ridlo/rl/rl-cw2/enduro/state.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, draw, scale)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_road_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__detectRoadGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         self._cars = self.__detectCars(\n\u001b[0;32m---> 51\u001b[0;31m             img * self.__getRoadMask(img, self._road_grid))\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getStateGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_road_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ridlo/rl/rl-cw2/enduro/state.pyc\u001b[0m in \u001b[0;36m__getRoadMask\u001b[0;34m(self, image, grid)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getRoadMask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;31m# Shring grid to avoid noise on the road edges\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ridlo/miniconda2/envs/rl/lib/python2.7/copy.pyc\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ridlo/miniconda2/envs/rl/lib/python2.7/copy.pyc\u001b[0m in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ridlo/miniconda2/envs/rl/lib/python2.7/copy.pyc\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ridlo/miniconda2/envs/rl/lib/python2.7/copy.pyc\u001b[0m in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class FunctionApproximationAgent(Agent):\n",
    "    def __init__(self):\n",
    "        super(FunctionApproximationAgent, self).__init__()\n",
    "        # The horizon defines how far the agent can see\n",
    "        self.horizon_row = 5\n",
    "\n",
    "        self.grid_cols = 10\n",
    "        # The state is defined as a tuple of the agent's x position and the\n",
    "        # x position of the closest opponent which is lower than the horizon,\n",
    "        # if any is present. There are four actions and so the Q(s, a) table\n",
    "        # has size of 10 * (10 + 1) * 4 = 440.\n",
    "        self.Q = np.ones((self.grid_cols, self.grid_cols + 1, 4))\n",
    "\n",
    "        # Add initial bias toward moving forward. This is not necessary,\n",
    "        # however it speeds up learning significantly, since the game does\n",
    "        # not provide negative reward if no cars have been passed by.\n",
    "        self.Q[:, :, 0] += 1.\n",
    "\n",
    "        # Helper dictionaries that allow us to move from actions to\n",
    "        # Q table indices and vice versa\n",
    "        self.idx2act = {i: a for i, a in enumerate(self.getActionsSet())}\n",
    "        self.act2idx = {a: i for i, a in enumerate(self.getActionsSet())}\n",
    "\n",
    "        # Learning rate\n",
    "        self.alpha = 0.0001\n",
    "        # Discounting factor\n",
    "        self.gamma = 0.9\n",
    "        # Exploration rate\n",
    "        self.epsilon = 0.01\n",
    "\n",
    "        # Log the obtained reward during learning\n",
    "        self.last_episode = 1\n",
    "        self.episode_log = np.zeros(6510) - 1.\n",
    "        self.log = []\n",
    "        \n",
    "        self.theta = np.random.uniform(low=0.1, high=0.5, size=2)\n",
    "        self.theta = np.array([0.1,0.1,0.1])\n",
    "        self.last_action = 0\n",
    "\n",
    "    def initialise(self, road, cars, speed, grid):\n",
    "        \"\"\" Called at the beginning of an episode. Use it to construct\n",
    "        the initial state.\n",
    "        \"\"\"\n",
    "        self.total_reward = 0\n",
    "        self.next_state2 = grid[:5]\n",
    "\n",
    "    def act(self):\n",
    "        \"\"\" Implements the decision making process for selecting\n",
    "        an action. Remember to store the obtained reward.\n",
    "        \"\"\"\n",
    "\n",
    "        self.state2 = self.next_state2\n",
    "\n",
    "        # If exploring\n",
    "        if np.random.uniform(0., 1.) < self.epsilon:\n",
    "            # Select a random action using softmax\n",
    "            idx = np.random.choice(4)#, p=probs)\n",
    "            self.action = self.idx2act[idx]\n",
    "            self.last_action = idx\n",
    "        else:\n",
    "            # Select the greedy action\n",
    "            idx = self.argmax_Qsa(self.state2)\n",
    "            #print idx\n",
    "            self.action = self.idx2act[idx]\n",
    "            self.last_action = idx\n",
    "\n",
    "        self.reward = self.move(self.action)\n",
    "        self.total_reward += self.reward\n",
    "\n",
    "    def sense(self, road, cars, speed, grid):\n",
    "        \"\"\" Constructs the next state from sensory signals.\n",
    "\n",
    "        Args:\n",
    "            road  -- 2-dimensional array containing [x, y] points\n",
    "                     in pixel coordinates of the road grid\n",
    "            cars  -- dictionary which contains the location and the size\n",
    "                     of the agent and the opponents in pixel coordinates\n",
    "            speed -- the relative speed of the agent with respect the others\n",
    "            gird  -- 2-dimensional numpy array containing the latest grid\n",
    "                     representation of the environment\n",
    "\n",
    "        For more information on the arguments have a look at the README.md\n",
    "        \"\"\"\n",
    "        self.next_state2 = grid[:5]\n",
    "\n",
    "        # Visualise the environment grid\n",
    "        #cv2.imshow(\"Enduro\", self._image)\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\" Performs the learning procedure. It is called after act() and\n",
    "        sense() so you have access to the latest tuple (s, s', a, r).\n",
    "        \"\"\"\n",
    "        #print \"last action = \"+str(self.last_action)\n",
    "        #print \"state2 = \"+str(self.state2)\n",
    "        _max_Qsa = self.max_Qsa(self.next_state2)\n",
    "        self.phi = [phi_position(self.state2[0], self.last_action), phi_action(self.last_action),\n",
    "                   phi_opponent(self.state2[0], self.state2[1:], self.last_action)]\n",
    "        \n",
    "        Q_sa = np.dot(self.theta.T, self.phi)\n",
    "        #print str(Q_sa)+\" = \"+str(self.theta)+' * '+str(self.phi)\n",
    "        \n",
    "        \n",
    "        '''print \"grad = \" + str(np.dot(self.alpha*(self.reward + self.gamma * _max_Qsa - Q_sa),\n",
    "                                    self.phi)) + \" = \" + str(\n",
    "            self.alpha*(self.reward + self.gamma * _max_Qsa - Q_sa)) + \" * \" + str(\n",
    "        self.phi)'''\n",
    "        \n",
    "        #self.theta = self.theta + np.dot(self.alpha*(self.reward + self.gamma * _max_Qsa - Q_sa), self.phi)\n",
    "        \n",
    "        #print \"Updated theta = \"+str(self.theta)\n",
    "        \n",
    "        #Q_sa = self.Q[self.state[0], self.state[1], self.act2idx[self.action]]\n",
    "\n",
    "        # Calculate the updated state action value\n",
    "        #Q_sa_new = Q_sa + self.alpha * (self.reward + self.gamma * self.maxQsa(self.next_state) - Q_sa)\n",
    "\n",
    "        # Write the updated value\n",
    "        #self.Q[self.state[0], self.state[1], self.act2idx[self.action]] = Q_sa_new\n",
    "        pass\n",
    "\n",
    "    def callback(self, learn, episode, iteration):\n",
    "        \"\"\" Called at the end of each timestep for reporting/debugging purposes.\n",
    "        \"\"\"\n",
    "        if not iteration%1000:\n",
    "            print \"{0}/{1}: {2}\".format(episode, iteration, self.total_reward)\n",
    "\n",
    "        #if not episode % 100:\n",
    "        cv2.imshow(\"Enduro\", self._image)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    def QsaAll(self, grid):\n",
    "        self.allQsa = []\n",
    "        for each in [0,1,2,3]:\n",
    "            self.phi = [phi_position(grid[0], each), phi_action(each), \n",
    "                        phi_opponent(grid[0], grid[1:], each)]\n",
    "            _ = np.dot(self.theta.T, self.phi)\n",
    "            self.allQsa.append(_)\n",
    "        #print self.allQsa\n",
    "        #print 'MAX = ' + str(np.max(self.allQsa))\n",
    "        #print 'ARD MAX = ' + str(np.argmax(self.allQsa))\n",
    "        return self.allQsa\n",
    "    \n",
    "    def max_Qsa(self, grid):\n",
    "        return np.max(self.QsaAll(grid))\n",
    "    def argmax_Qsa(self, grid):\n",
    "        return np.argmax(self.QsaAll(grid))\n",
    "    \n",
    "    def maxQsa(self, state):\n",
    "        return np.max(self.Q[state[0], state[1], :])\n",
    "\n",
    "    def argmaxQsa(self, state):\n",
    "        return np.argmax(self.Q[state[0], state[1], :])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    a = FunctionApproximationAgent()\n",
    "    a.run(True, episodes=2000, draw=True)\n",
    "    print 'Total reward: ' + str(a.total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2000: 5\n",
      "1/3000: 14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-225-832213c6f858>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionApproximationAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'Total reward: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ridlo/rl/rl-cw2/enduro/agent.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, learn, episodes, draw)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0;31m# Update the environment grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mroad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_image\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ridlo/rl/rl-cw2/enduro/state.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, draw, scale)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_road_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__detectRoadGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         self._cars = self.__detectCars(\n\u001b[0;32m---> 51\u001b[0;31m             img * self.__getRoadMask(img, self._road_grid))\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getStateGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_road_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ridlo/rl/rl-cw2/enduro/state.pyc\u001b[0m in \u001b[0;36m__detectCars\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# Detect the contour of the player's car\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         _, thresh = cv2.threshold(\n\u001b[0;32m--> 141\u001b[0;31m             cv2.cvtColor(image * mask, cv2.COLOR_BGR2GRAY), 170, 255, 0)\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_cv3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
