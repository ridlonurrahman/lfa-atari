{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from enduro.agent import Agent\n",
    "from enduro.action import Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phi_position(grid, act):\n",
    "    # Feature representing position and act\n",
    "    grid = grid.tolist()\n",
    "    if grid.index(2)>5:\n",
    "        if act == 0:\n",
    "            return 1\n",
    "        elif act == 2:\n",
    "            return 5\n",
    "        else:\n",
    "            return 1\n",
    "    elif grid.index(2)<4:\n",
    "        if act == 0:\n",
    "            return 1\n",
    "        elif act == 1:\n",
    "            return 5\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        if act == 0:\n",
    "            return 10\n",
    "        elif act == 3:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "\n",
    "def phi_action(act):\n",
    "    # Feature representing act, turn off this feature for a better reward LOL\n",
    "    return 0\n",
    "    if act == 1 or act == 2:\n",
    "        return 2\n",
    "    elif act == 0:\n",
    "        return 5\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def phi_opponent(grid_agent, grid_opponent, act):\n",
    "    # Feature representing opponents\n",
    "    \n",
    "    opponents = np.where(grid_opponent==1)[1]\n",
    "    agent = np.where(grid_agent==2)[0][0]\n",
    "    \n",
    "    if any(x>=0 and x<3 for x in opponents-agent):\n",
    "        if act == 2:\n",
    "            return 20\n",
    "        else:\n",
    "            return 0\n",
    "    elif any(x<=0 and x>-3 for x in opponents-agent):\n",
    "        if act == 1:\n",
    "            return 20\n",
    "        else:\n",
    "            return 0      \n",
    "    else:\n",
    "        if act == 3:\n",
    "            return 0\n",
    "        else:\n",
    "            return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunctionApproximationAgent(Agent):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FunctionApproximationAgent, self).__init__()\n",
    "        # The horizon defines how far the agent can see\n",
    "        self.horizon_row = 5\n",
    "\n",
    "        self.grid_cols = 10\n",
    "        # The state is defined as a tuple of the agent's x position and the\n",
    "        # x position of the closest opponent which is lower than the horizon,\n",
    "        # if any is present. There are four actions and so the Q(s, a) table\n",
    "        # has size of 10 * (10 + 1) * 4 = 440.\n",
    "        self.Q = np.ones((self.grid_cols, self.grid_cols + 1, 4))\n",
    "\n",
    "        # Add initial bias toward moving forward. This is not necessary,\n",
    "        # however it speeds up learning significantly, since the game does\n",
    "        # not provide negative reward if no cars have been passed by.\n",
    "        self.Q[:, :, 0] += 1.\n",
    "\n",
    "        # Helper dictionaries that allow us to move from actions to\n",
    "        # Q table indices and vice versa\n",
    "        self.idx2act = {i: a for i, a in enumerate(self.getActionsSet())}\n",
    "        self.act2idx = {a: i for i, a in enumerate(self.getActionsSet())}\n",
    "\n",
    "        # Learning rate\n",
    "        self.alpha = 0.00001\n",
    "        # Discounting factor\n",
    "        self.gamma = 0.9\n",
    "        # Exploration rate\n",
    "        self.epsilon = 0.01\n",
    "\n",
    "        # Log the obtained reward during learning\n",
    "        self.last_episode = 1\n",
    "        self.episode_log = np.zeros(6510) - 1.\n",
    "        self.log = []\n",
    "        \n",
    "        self.theta = np.random.uniform(low=0.1, high=0.5, size=2)\n",
    "        self.theta = np.array([0.1,0.1,0.1])\n",
    "        self.last_action = 0\n",
    "        self.total_reward = 0\n",
    "\n",
    "    def initialise(self, road, cars, speed, grid):\n",
    "        \"\"\" Called at the beginning of an episode. Use it to construct\n",
    "        the initial state.\n",
    "        \"\"\"\n",
    "        print 'Total reward = '+str(self.total_reward)+\"; Theta = \"+str(self.theta)\n",
    "        self.total_reward = 0\n",
    "        self.next_state2 = grid[:5]\n",
    "\n",
    "    def act(self):\n",
    "        \"\"\" Implements the decision making process for selecting\n",
    "        an action. Remember to store the obtained reward.\n",
    "        \"\"\"\n",
    "\n",
    "        self.state2 = self.next_state2\n",
    "\n",
    "        # If exploring\n",
    "        if np.random.uniform(0., 1.) < self.epsilon:\n",
    "            # Select a random action using softmax\n",
    "            idx = np.random.choice(4)#, p=probs)\n",
    "            self.action = self.idx2act[idx]\n",
    "            self.last_action = idx\n",
    "        else:\n",
    "            # Select the greedy action\n",
    "            idx = self.argmax_Qsa(self.state2)\n",
    "            #print idx\n",
    "            self.action = self.idx2act[idx]\n",
    "            self.last_action = idx\n",
    "\n",
    "        self.reward = self.move(self.action)\n",
    "        self.total_reward += self.reward\n",
    "\n",
    "    def sense(self, road, cars, speed, grid):\n",
    "        \"\"\" Constructs the next state from sensory signals.\n",
    "\n",
    "        Args:\n",
    "            road  -- 2-dimensional array containing [x, y] points\n",
    "                     in pixel coordinates of the road grid\n",
    "            cars  -- dictionary which contains the location and the size\n",
    "                     of the agent and the opponents in pixel coordinates\n",
    "            speed -- the relative speed of the agent with respect the others\n",
    "            gird  -- 2-dimensional numpy array containing the latest grid\n",
    "                     representation of the environment\n",
    "\n",
    "        For more information on the arguments have a look at the README.md\n",
    "        \"\"\"\n",
    "        self.next_state2 = grid[:5]\n",
    "\n",
    "        # Visualise the environment grid\n",
    "        #cv2.imshow(\"Enduro\", self._image)\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\" Performs the learning procedure. It is called after act() and\n",
    "        sense() so you have access to the latest tuple (s, s', a, r).\n",
    "        \"\"\"\n",
    "        _max_Qsa = self.max_Qsa(self.next_state2)\n",
    "        self.phi = [phi_position(self.state2[0], self.last_action), phi_action(self.last_action),\n",
    "                   phi_opponent(self.state2[0], self.state2[1:], self.last_action)]\n",
    "        \n",
    "        Q_sa = np.dot(self.theta.T, self.phi)\n",
    "        \n",
    "        self.theta = self.theta + np.dot(self.alpha*(self.reward + self.gamma * _max_Qsa - Q_sa), self.phi)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def callback(self, learn, episode, iteration):\n",
    "        \"\"\" Called at the end of each timestep for reporting/debugging purposes.\n",
    "        \"\"\"\n",
    "        cv2.imshow(\"Enduro\", self._image)\n",
    "        cv2.waitKey(20)\n",
    "\n",
    "    def QsaAll(self, grid):\n",
    "        self.allQsa = []\n",
    "        for each in [0,1,2,3]:\n",
    "            self.phi = [phi_position(grid[0], each), phi_action(each), \n",
    "                        phi_opponent(grid[0], grid[1:], each)]\n",
    "            _ = np.dot(self.theta.T, self.phi)\n",
    "            self.allQsa.append(_)\n",
    "        return self.allQsa\n",
    "    \n",
    "    def max_Qsa(self, grid):\n",
    "        # Calculates max Qsa\n",
    "        return np.max(self.QsaAll(grid))\n",
    "    \n",
    "    def argmax_Qsa(self, grid):\n",
    "        # Calculates argmax Qsa\n",
    "        return np.argmax(self.QsaAll(grid))\n",
    "    \n",
    "    '''\n",
    "    # Obsolete function\n",
    "    \n",
    "    def maxQsa(self, state):\n",
    "        return np.max(self.Q[state[0], state[1], :])\n",
    "\n",
    "    def argmaxQsa(self, state):\n",
    "        return np.argmax(self.Q[state[0], state[1], :])\n",
    "    '''\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    a = FunctionApproximationAgent()\n",
    "    a.run(True, episodes=2000, draw=True)\n",
    "    print 'Total reward: ' + str(a.total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
